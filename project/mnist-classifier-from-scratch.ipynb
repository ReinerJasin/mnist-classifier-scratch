{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this python notebook, I will show you how you can develop your own neural network to classify MNIST image dataset.\n",
    "\n",
    "The theoretical explanations and formulas presented in this notebook are based on insights provided by ChatGPT, which I have carefully reviewed and adjusted as the author to ensure clarity and accuracy,\n",
    "\n",
    "The code implementation is written from scratch with every aspect of the neural network being manually coded to provide a deeper understanding of how these models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MNIST dataset** (Modified National Institute of Standards and Technology) is a collection of **70,000 grayscale images** of handwritten digits (0-9). Each image is **28×28 pixels**, making it a common benchmark for image classification tasks.\n",
    "\n",
    "* **Training set**: 60,000 images\n",
    "* **Test set**: 10,000 images\n",
    "\n",
    "This dataset is widely used to test the performance of machine learning models, particularly in neural network-based classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Networks Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a computational model inspired by the human brain, consisting of neurons organized into layers. The key layers are:\n",
    "\n",
    "* Input Layer: Takes the pixel values as input.\n",
    "* Hidden Layers: Process the input using weights and activation functions.\n",
    "* Output Layer: Produces the final classification (digits 0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neuron Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron is a computational unit in the network that processes inputs, applies weights, and produces an output. The neuron performs the following:\n",
    "\n",
    "Compute Weighted Sum:\n",
    "$$ z = Wx + b $$\n",
    "\n",
    "where:\n",
    "* 𝑥 = input vector\n",
    "* 𝑊 = weight matrix\n",
    "* 𝑏 = bias\n",
    "* 𝑧 = weighted sum (logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for initializing the neural network structure here\n",
    "# class neuron:\n",
    "#     def __init__:\n",
    "#         self."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions introduce non-linearity into the network, which is crucial for learning complex patterns.\n",
    "\n",
    "> Without non-linearity, a neural network, no matter how many layers it has, would essentially behave like a single linear transformation. This is because a series of matrix multiplications is still just another matrix multiplication — effectively reducing the network to a simple linear model. By adding non-linearity, we enable the network to model intricate relationships in the data, allowing it to perform more sophisticated tasks.\n",
    "\n",
    "Common activation functions include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ReLU (Rectified Linear Unit):\n",
    "\n",
    "$$ f(z) = \\max(0, z) $$\n",
    "\n",
    "* Used in hidden layers\n",
    "* It introduces non-linearity and helps with the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0.1, 0.5, 1.7, 1.2, 0, 0.2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for ReLU function\n",
    "def relu(scores):\n",
    "    # Sample Input\n",
    "    # scores = [-1.6, -0.9,-1.1, -2.5, 0.1, 0.5, 1.7, 1.2, -0.3, 0.2]\n",
    "\n",
    "    relu_value = [max(0, x) for x in scores]\n",
    "    \n",
    "    # relu_value = [x if x > 0 else 0 for x in scores]\n",
    "\n",
    "    return relu_value\n",
    "\n",
    "relu([-1.6, -0.9,-1.1, -2.5, 0.1, 0.5, 1.7, 1.2, -0.3, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Softmax (for output layer):\n",
    "\n",
    "$$ \\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} $$\n",
    " \n",
    "* Converts logits into probabilities for classification. Softmax ensures that the output values sum to 1, making them interpretable as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.01389170760009839),\n",
       " np.float64(0.027974463791086333),\n",
       " np.float64(0.022903553806628847),\n",
       " np.float64(0.0056479468321878985),\n",
       " np.float64(0.07604247658419551),\n",
       " np.float64(0.11344204463811085),\n",
       " np.float64(0.37664085215282667),\n",
       " np.float64(0.22844422453098234),\n",
       " np.float64(0.05097279640458196),\n",
       " np.float64(0.08403993365930129)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for Softmax layer\n",
    "def softmax(scores):\n",
    "    # Sample Input\n",
    "    # scores = [-1.6, -0.9,-1.1, -2.5, 0.1, 0.5, 1.7, 1.2, -0.3, 0.2]\n",
    "\n",
    "    # Without full comprehension\n",
    "    # scores_exp = np.exp(scores)\n",
    "    # scores_sum = np.sum(scores_exp)\n",
    "\n",
    "    # softmax_value = [x/scores_sum for x in scores_exp]\n",
    "\n",
    "    # With full comprehension\n",
    "    softmax_value = [x/np.sum(np.exp(scores)) for x in np.exp(scores)]\n",
    "    \n",
    "    return softmax_value\n",
    "\n",
    "softmax([-1.6, -0.9,-1.1, -2.5, 0.1, 0.5, 1.7, 1.2, -0.3, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sigmoid (optional, not used in this code)\n",
    "$$ f(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "* Often used for binary classification but not common for multi-class problems like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m     sigmoid_value = \u001b[32m1\u001b[39m/(\u001b[32m1\u001b[39m+np.exp(-scores))\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sigmoid_value\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36msigmoid\u001b[39m\u001b[34m(scores)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msigmoid\u001b[39m(scores):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Sample Input\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# scores = [-1.6, -0.9,-1.1, -2.5, 0.1, 0.5, 1.7, 1.2, -0.3, 0.2]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# With full comprehension\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     sigmoid_value = \u001b[32m1\u001b[39m/(\u001b[32m1\u001b[39m+np.exp(\u001b[43m-\u001b[49m\u001b[43mscores\u001b[49m))\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sigmoid_value\n",
      "\u001b[31mTypeError\u001b[39m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "# Code for Softmax layer\n",
    "def sigmoid(scores):\n",
    "    # Sample Input\n",
    "    # scores = [-1.6, -0.9,-1.1, -2.5, 0.1, 0.5, 1.7, 1.2, -0.3, 0.2]\n",
    "\n",
    "    # Without full comprehension\n",
    "    # scores_exp = np.exp(scores)\n",
    "    # scores_sum = np.sum(scores_exp)\n",
    "\n",
    "    # softmax_value = [x/scores_sum for x in scores_exp]\n",
    "\n",
    "    # With full comprehension\n",
    "    sigmoid_value = 1/(1+np.exp(-)) for x in scores\n",
    "    \n",
    "    return sigmoid_value\n",
    "\n",
    "sigmoid([1.7, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how well the network is performing, we use a loss function that measures the difference between the predicted outputs and the true labels. In the case of classification, we typically use Cross-Entropy Loss:\n",
    "\n",
    "For a single training example:\n",
    "$$ L = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where:\n",
    "* $$ C \\text{= number of classes (10 in MNIST)} $$\n",
    "* $$ y_{i} \\text{= true label (one-hat encoded)} $$\n",
    "* $$ \\hat{y}_i \\text{= predicted probability (from softmax)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
